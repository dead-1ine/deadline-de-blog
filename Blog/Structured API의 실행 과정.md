# Spark - Structured API의 실행 과정

이 글은 Orielly - Spark, Definitive Guide를 참조하였습니다.

책의 목차 중 2부 구조적 API: DataFrame, SQL, Dataset의 4.4 구조적 API의 실행 과정 원문에서 보다 이해하기 쉽도록 용어에 대한 설명을 덧붙이고 그림을 약간 변경하여 글을 작성하였습니다.

이 글에서 나오는 용어

- 구조적 API(Structured API)
- 클러스터(Cluster)
- 논리적 실행 계획(Logical Plan)
- 물리적 실행 계획(Physical Plan)
- 지연 평가(Lazy Evaluation):
    - Transformation
    - Action
- 카탈리스트 옵티마이저(Catalyst Optimizer)
- 카탈로그(Catalog)

Spark 코드는 Lazy Evaluation을 통해 Transformation과 Action 메서드에 따라 실행 계획을 생성합니다. Spark의 실행 계획을 잘 보아야 더 나은 코드를 작성할 수 있게 되며, Lazy Evaluation이 갖는 이점을 이해할 수 있습니다. Spark에서 Lazy Evaluation과 실행 계획에 대한 연관성 및 동작 방식에 대해 아래에서 상세히 설명하겠습니다.

아래는 구조적 API 쿼리가 사용자의 코드에서 실행 코드로 변환되는 과정입니다.

1. 사용자가 DataFramem/Dataset/SQL을 이용하여 코드를 작성
2. 작성된 코드는 console이나 spark-submit shell script로 실행
3. Spark의 Catalyst Optimizer가 코드를 논리적 실행 계획으로 변환
4. Spark의 Catalyst Optimizer가 논리적 실행 계획을 물리적 실행 계획으로 변환하며 이 과정 사이에 메서드 간 최적화가 가능한지 확인
5. Spark가 클러스터에서 물리적 실행 계획을 실행 후 결과를 반환

![그림 1. Catalyst Optimizer](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ae3244d9-c933-4fde-a03a-94e3194ecbe3/IMG_0016.jpg)

그림 1. Catalyst Optimizer

### 논리적 실행계획

사용자가 작성한 Spark 코드는 Spark의 Catalyst Optimizer를 거쳐 논리적 실행 계획으로 변환됩니다.

각 단계가 실행되는 프로세스는 아래의 그림과 같습니다.

![그림 2. 구조적 API의 논리적 실행 계획 수립 과정](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/eac91bbe-ac5c-4924-b6e9-707366ed8fe9/IMG_0015.jpg)

그림 2. 구조적 API의 논리적 실행 계획 수립 과정

논리적 실행 계획 단계에서는 추상적 트랜스포메이션만을 표현합니다. 아래는 각 단계에 대한 설명입니다.

- CODE

    사용자가 작성한 Spark 코드입니다.

- 검증 전 논리적 실행 계획(unresolved logical plan)

    사용자의 Spark 코드를 논리적 실행계획으로 변환하기 위한 첫 번째 단계입니다. 이 단계에서는 사용자의 다양한 표현식으로 작성된 Spark 코드를 최적화합니다. 이 때 드라이버나 익스큐터에 대한 정보는 고려하지 않은 최적화가 이루어집니다. 또한 코드의 유효성이나 spark.read()에 대상이 되는 데이터의 테이블이나 컬럼의 존재 여부만을 판단하는 과정이므로 아직 실행 계획을 검증하지는 않은 상태입니다.

- 분석(Spark의 Anaylzer를 사용)

    대상 데이터의 테이블과 컬럼을 검증하는 단계입니다. 이 때 메타스토어의 역할을 하는 카탈로그와 모든 테이블의 저장소 그리고 DataFrame의 정보를 활용합니다.

    - 카탈로그

        카탈로그는 메타스토어와 같이 동작하는 인터페이스입니다. 카탈로그에는 데이터베이스의 카탈로그와 외부 테이블, 함수, 테이블의 컬럼 그리고 임시 뷰에 대한 정보를 담고 있는 Spark SQL의 가장 높은 추상화 단계입니다.

    필요한 테이블이나 컬럼이 카탈로그에 없다면 검증 전 논리적 실행 계획이 생성되지 않습니다.

- 검증된 논리적 실행 계획

    앞선 단계인 분석에서 반환된 단계의 실행 계획입니다. 이 실행 계획은 카탈리스트 옵티마이저로 전달됩니다.

- 논리적 최적화

    검증된 논리적 실행 계획을 카탈리스트 옵티마이저가 최적화합니다.

    - 카탈리스트 옵티마이저

        조건절 푸시 다운(predicate pushing down)이나 선택절 구문을 이용해 논리적 실행 계획을 최적화하는 규칙의 모음입니다. 필요한 경우 도메인에 최적화된 규칙을 적용할 수 있는 카탈리스트 옵티마이저의 확장형 패키지를 만들 수도 있습니다.

- 최적화된 논리적 실행 계획

    논리적 실행 계획의 최종 결과물로, 앞선 논리적 최적화 단계에서 검증된 논리적 실행 계획이 카탈리스트 옵티마이저를 통해 실행 계획이 최적화되어 반환된 형태입니다.

### 물리적 실행 계획

Spark 실행 계획이라고도 불리는 물리적 실행 계획은 논리적 실행 계획을 변환하여 클러스터 환경에서 실행하는 방법을 정의합니다.

아래 그림과 같이 최적화된 논리적 실행 계획을 이용하여 여러개의 물리적 실행 계획을 생성합니다. 생성된 여러 개의 물리적 실행 계획은 비용 모델을 이용하여 서로를 비교한 후 최적의 전략을 선택하여 최적의 물리적 실행 계획을 반환합니다.

![그림 3. 물리적 실행 계획 수립 과정](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8b9c1d1a-58e3-4322-b60e-cd402d3cd4f0/IMG_0017.jpg)

그림 3. 물리적 실행 계획 수립 과정

물리적 실행 계획들의 비용을 비교하는 한 가지 예는 사용하려는 테이블의 크기나 파티션 수 등의 물리적 속성을 고려하여 지정된 조인 연산 수행에 필요한 비용을 계산하고 비교하는 것입니다.

비용 모델을 적용하여 반환된 최적의 물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환됩니다.

### 실행 계획 정리(Structured API → RDD)

전체 과정으로 보았을 때, 제일 첫 단계인 사용자의 Spark 코드는 DataFrame, Dataset, SQL로 작성되어 있습니다.

Spark Strcuted API(DataFrame, Dataset, SQL)로 작성된 사용자의 코드는 논리적 실행 계획에서 물리적 실행 계획의 단계를 거쳐 클러스터에서 실행되기 위해 RDD 트랜스포메이션으로 컴파일됩니다. 이러한 Spark의 실행 단계에 따라 Spark를 ‘컴파일러’라고 부르기도 합니다.

### 실행

Spark는 Structured API로 작성된 사용자 코드를 각 단계를 거쳐 최적의 물리적 실행 계획을 선정하고 이를 저수준 프로그래밍 인터페이스인 RDD로 컴파일합니다. 컴파일된 RDD는 클러스터에서 실행되며 런타임 시 Task나 Stage를 제거할 수 있는 자바 바이트 코드를 생성하여 추가적인 최적화를 수행합니다. 마지막으로 Spark는 처리 결과를 사용자에게 반환합니다.

### 정리

일반적으로 사용자는 사용하기 쉬운 Structured API를 사용하여 Spark 코드를 작성합니다. 작성된 코드는 위에서 설명한 각 단계를 거쳐 최종적으로 클러스터에서 실행될 RDD로 변환됩니다. 또한, RDD가 실행되는 런타임에서 Spark는 JVM 환경에서 동작하기 때문에 이 단계에서 추가적인 최적화 단계가 수행됩니다.